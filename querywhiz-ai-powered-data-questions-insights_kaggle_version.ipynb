{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11268405,"sourceType":"datasetVersion","datasetId":7043791}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yanelly/querywhiz-ai-powered-data-questions-insights?scriptVersionId=235364428\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 🤖 GenAI for Data Analysis: Ask Questions, Get SQL, Understand Results\n\nWelcome! This project demonstrates how to use **Google Gemini AI** to enhance the data analysis process with:\n- AI-generated **questions and SQL queries**\n- Natural language **summaries of query results**\n- Support for **custom CSV uploads** or a built-in **Kaggle dataset**\n\n🎯 Goal: Show how GenAI can act like a smart analyst assistant for any dataset.\n","metadata":{}},{"cell_type":"markdown","source":"# 1. 🔧 Setup: Libraries, API Key, and Imports","metadata":{}},{"cell_type":"code","source":"# Gemini + API Setup\nfrom google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:46.072297Z","iopub.execute_input":"2025-04-22T05:31:46.072567Z","iopub.status.idle":"2025-04-22T05:31:47.345027Z","shell.execute_reply.started":"2025-04-22T05:31:46.072547Z","shell.execute_reply":"2025-04-22T05:31:47.344115Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n  warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# For API retry handling\nfrom google.api_core import retry","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:47.34656Z","iopub.execute_input":"2025-04-22T05:31:47.347009Z","iopub.status.idle":"2025-04-22T05:31:47.647853Z","shell.execute_reply.started":"2025-04-22T05:31:47.346982Z","shell.execute_reply":"2025-04-22T05:31:47.646959Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Displaying Data\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:47.648785Z","iopub.execute_input":"2025-04-22T05:31:47.649178Z","iopub.status.idle":"2025-04-22T05:31:48.051876Z","shell.execute_reply.started":"2025-04-22T05:31:47.649156Z","shell.execute_reply":"2025-04-22T05:31:48.050733Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# SQLite DB\nimport sqlite3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:48.053514Z","iopub.execute_input":"2025-04-22T05:31:48.053959Z","iopub.status.idle":"2025-04-22T05:31:48.0578Z","shell.execute_reply.started":"2025-04-22T05:31:48.053935Z","shell.execute_reply":"2025-04-22T05:31:48.05694Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Librairies for the inputed csv:\nfrom IPython.display import display\nimport ipywidgets as widgets\nimport io","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:48.058622Z","iopub.execute_input":"2025-04-22T05:31:48.058912Z","iopub.status.idle":"2025-04-22T05:31:48.216117Z","shell.execute_reply.started":"2025-04-22T05:31:48.058892Z","shell.execute_reply":"2025-04-22T05:31:48.215221Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# API Error \nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:48.216905Z","iopub.execute_input":"2025-04-22T05:31:48.217159Z","iopub.status.idle":"2025-04-22T05:31:48.222788Z","shell.execute_reply.started":"2025-04-22T05:31:48.217139Z","shell.execute_reply":"2025-04-22T05:31:48.221333Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# getting API key\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:48.22366Z","iopub.execute_input":"2025-04-22T05:31:48.223937Z","iopub.status.idle":"2025-04-22T05:31:48.328502Z","shell.execute_reply.started":"2025-04-22T05:31:48.223916Z","shell.execute_reply":"2025-04-22T05:31:48.327721Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 2. 📂 Load a Dataset (Kaggle OR Upload your own CSV)","metadata":{}},{"cell_type":"markdown","source":"## Loading kaggle data","metadata":{}},{"cell_type":"code","source":"# Data Set: \ndata = pd.read_csv('/kaggle/input/amazon-sales-2025/amazon_sales_data 2025.csv')\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:48.329359Z","iopub.execute_input":"2025-04-22T05:31:48.329705Z","iopub.status.idle":"2025-04-22T05:31:48.375028Z","shell.execute_reply.started":"2025-04-22T05:31:48.329646Z","shell.execute_reply":"2025-04-22T05:31:48.374182Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    Order ID      Date        Product     Category  Price  Quantity  \\\n0    ORD0001  14-03-25  Running Shoes     Footwear     60         3   \n1    ORD0002  20-03-25     Headphones  Electronics    100         4   \n2    ORD0003  15-02-25  Running Shoes     Footwear     60         2   \n3    ORD0004  19-02-25  Running Shoes     Footwear     60         3   \n4    ORD0005  10-03-25     Smartwatch  Electronics    150         3   \n..       ...       ...            ...          ...    ...       ...   \n245  ORD0246  17-03-25        T-Shirt     Clothing     20         2   \n246  ORD0247  30-03-25          Jeans     Clothing     40         1   \n247  ORD0248  05-03-25        T-Shirt     Clothing     20         2   \n248  ORD0249  08-03-25     Smartwatch  Electronics    150         3   \n249  ORD0250  19-02-25     Smartphone  Electronics    500         4   \n\n     Total Sales  Customer Name Customer Location Payment Method     Status  \n0            180     Emma Clark          New York     Debit Card  Cancelled  \n1            400  Emily Johnson     San Francisco     Debit Card    Pending  \n2            120       John Doe            Denver     Amazon Pay  Cancelled  \n3            180  Olivia Wilson            Dallas    Credit Card    Pending  \n4            450     Emma Clark          New York     Debit Card    Pending  \n..           ...            ...               ...            ...        ...  \n245           40  Daniel Harris             Miami     Debit Card  Cancelled  \n246           40  Sophia Miller            Dallas     Debit Card  Cancelled  \n247           40    Chris White            Denver     Debit Card  Cancelled  \n248          450  Emily Johnson          New York     Debit Card  Cancelled  \n249         2000  Emily Johnson           Seattle     Amazon Pay  Completed  \n\n[250 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Order ID</th>\n      <th>Date</th>\n      <th>Product</th>\n      <th>Category</th>\n      <th>Price</th>\n      <th>Quantity</th>\n      <th>Total Sales</th>\n      <th>Customer Name</th>\n      <th>Customer Location</th>\n      <th>Payment Method</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ORD0001</td>\n      <td>14-03-25</td>\n      <td>Running Shoes</td>\n      <td>Footwear</td>\n      <td>60</td>\n      <td>3</td>\n      <td>180</td>\n      <td>Emma Clark</td>\n      <td>New York</td>\n      <td>Debit Card</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ORD0002</td>\n      <td>20-03-25</td>\n      <td>Headphones</td>\n      <td>Electronics</td>\n      <td>100</td>\n      <td>4</td>\n      <td>400</td>\n      <td>Emily Johnson</td>\n      <td>San Francisco</td>\n      <td>Debit Card</td>\n      <td>Pending</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORD0003</td>\n      <td>15-02-25</td>\n      <td>Running Shoes</td>\n      <td>Footwear</td>\n      <td>60</td>\n      <td>2</td>\n      <td>120</td>\n      <td>John Doe</td>\n      <td>Denver</td>\n      <td>Amazon Pay</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ORD0004</td>\n      <td>19-02-25</td>\n      <td>Running Shoes</td>\n      <td>Footwear</td>\n      <td>60</td>\n      <td>3</td>\n      <td>180</td>\n      <td>Olivia Wilson</td>\n      <td>Dallas</td>\n      <td>Credit Card</td>\n      <td>Pending</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ORD0005</td>\n      <td>10-03-25</td>\n      <td>Smartwatch</td>\n      <td>Electronics</td>\n      <td>150</td>\n      <td>3</td>\n      <td>450</td>\n      <td>Emma Clark</td>\n      <td>New York</td>\n      <td>Debit Card</td>\n      <td>Pending</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>ORD0246</td>\n      <td>17-03-25</td>\n      <td>T-Shirt</td>\n      <td>Clothing</td>\n      <td>20</td>\n      <td>2</td>\n      <td>40</td>\n      <td>Daniel Harris</td>\n      <td>Miami</td>\n      <td>Debit Card</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>ORD0247</td>\n      <td>30-03-25</td>\n      <td>Jeans</td>\n      <td>Clothing</td>\n      <td>40</td>\n      <td>1</td>\n      <td>40</td>\n      <td>Sophia Miller</td>\n      <td>Dallas</td>\n      <td>Debit Card</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>ORD0248</td>\n      <td>05-03-25</td>\n      <td>T-Shirt</td>\n      <td>Clothing</td>\n      <td>20</td>\n      <td>2</td>\n      <td>40</td>\n      <td>Chris White</td>\n      <td>Denver</td>\n      <td>Debit Card</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>ORD0249</td>\n      <td>08-03-25</td>\n      <td>Smartwatch</td>\n      <td>Electronics</td>\n      <td>150</td>\n      <td>3</td>\n      <td>450</td>\n      <td>Emily Johnson</td>\n      <td>New York</td>\n      <td>Debit Card</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>ORD0250</td>\n      <td>19-02-25</td>\n      <td>Smartphone</td>\n      <td>Electronics</td>\n      <td>500</td>\n      <td>4</td>\n      <td>2000</td>\n      <td>Emily Johnson</td>\n      <td>Seattle</td>\n      <td>Amazon Pay</td>\n      <td>Completed</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 11 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Loading input data","metadata":{}},{"cell_type":"markdown","source":"> 👉 **Tip**: You can upload your own `.csv` file using the widget below. If you skip this step, the notebook will use a default Amazon Sales dataset.\n","metadata":{}},{"cell_type":"code","source":"# Upload CSV widget\nupload = widgets.FileUpload(accept='.csv', multiple=False)\ndisplay(upload)\n\n# Ask user if they want to upload a CSV\nuse_upload = input(\"📤 Do you want to upload your own CSV file? (y/n): \").lower()\n\nif use_upload == 'y':\n    # Show the upload widget\n    upload = widgets.FileUpload(accept='.csv', multiple=False)\n    display(upload)\n    print(\"📤 Please upload your file using the widget above, then run the next cell.\")\nelse:\n    print(\"📂 Skipping upload. We'll use the default dataset instead.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:48.375953Z","iopub.execute_input":"2025-04-22T05:31:48.376213Z","iopub.status.idle":"2025-04-22T05:31:54.330773Z","shell.execute_reply.started":"2025-04-22T05:31:48.376194Z","shell.execute_reply":"2025-04-22T05:31:54.329962Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='.csv', description='Upload')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b579b64b63ef47fb82fcf6aa1c725bb0"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"📤 Do you want to upload your own CSV file? (y/n):  y\n"},{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='.csv', description='Upload')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad038d627dc466db2cc7282f970a386"}},"metadata":{}},{"name":"stdout","text":"📤 Please upload your file using the widget above, then run the next cell.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Handle upload\ndef handle_upload():\n    if 'upload' in globals() and upload.value:\n        for file_info in upload.value:\n            content = file_info['content']  # Access the content attribute of the file info\n            df = pd.read_csv(io.BytesIO(content))\n            df.columns = [col.strip().replace(\" \", \"_\") for col in df.columns]\n            return df\n    return None  # No file uploaded\n\ndf = handle_upload()\n\nif df is None:\n    print(\"⚠️ No user file uploaded — using default Kaggle dataset instead.\")\n    df = pd.read_csv('/kaggle/input/amazon-sales-2025/amazon_sales_data 2025.csv')\n    df.columns = [col.strip().replace(\" \", \"_\") for col in df.columns]\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:54.33382Z","iopub.execute_input":"2025-04-22T05:31:54.334083Z","iopub.status.idle":"2025-04-22T05:31:54.352983Z","shell.execute_reply.started":"2025-04-22T05:31:54.334063Z","shell.execute_reply":"2025-04-22T05:31:54.352146Z"}},"outputs":[{"name":"stdout","text":"⚠️ No user file uploaded — using default Kaggle dataset instead.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"  Order_ID      Date        Product     Category  Price  Quantity  \\\n0  ORD0001  14-03-25  Running Shoes     Footwear     60         3   \n1  ORD0002  20-03-25     Headphones  Electronics    100         4   \n2  ORD0003  15-02-25  Running Shoes     Footwear     60         2   \n3  ORD0004  19-02-25  Running Shoes     Footwear     60         3   \n4  ORD0005  10-03-25     Smartwatch  Electronics    150         3   \n\n   Total_Sales  Customer_Name Customer_Location Payment_Method     Status  \n0          180     Emma Clark          New York     Debit Card  Cancelled  \n1          400  Emily Johnson     San Francisco     Debit Card    Pending  \n2          120       John Doe            Denver     Amazon Pay  Cancelled  \n3          180  Olivia Wilson            Dallas    Credit Card    Pending  \n4          450     Emma Clark          New York     Debit Card    Pending  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Order_ID</th>\n      <th>Date</th>\n      <th>Product</th>\n      <th>Category</th>\n      <th>Price</th>\n      <th>Quantity</th>\n      <th>Total_Sales</th>\n      <th>Customer_Name</th>\n      <th>Customer_Location</th>\n      <th>Payment_Method</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ORD0001</td>\n      <td>14-03-25</td>\n      <td>Running Shoes</td>\n      <td>Footwear</td>\n      <td>60</td>\n      <td>3</td>\n      <td>180</td>\n      <td>Emma Clark</td>\n      <td>New York</td>\n      <td>Debit Card</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ORD0002</td>\n      <td>20-03-25</td>\n      <td>Headphones</td>\n      <td>Electronics</td>\n      <td>100</td>\n      <td>4</td>\n      <td>400</td>\n      <td>Emily Johnson</td>\n      <td>San Francisco</td>\n      <td>Debit Card</td>\n      <td>Pending</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ORD0003</td>\n      <td>15-02-25</td>\n      <td>Running Shoes</td>\n      <td>Footwear</td>\n      <td>60</td>\n      <td>2</td>\n      <td>120</td>\n      <td>John Doe</td>\n      <td>Denver</td>\n      <td>Amazon Pay</td>\n      <td>Cancelled</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ORD0004</td>\n      <td>19-02-25</td>\n      <td>Running Shoes</td>\n      <td>Footwear</td>\n      <td>60</td>\n      <td>3</td>\n      <td>180</td>\n      <td>Olivia Wilson</td>\n      <td>Dallas</td>\n      <td>Credit Card</td>\n      <td>Pending</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ORD0005</td>\n      <td>10-03-25</td>\n      <td>Smartwatch</td>\n      <td>Electronics</td>\n      <td>150</td>\n      <td>3</td>\n      <td>450</td>\n      <td>Emma Clark</td>\n      <td>New York</td>\n      <td>Debit Card</td>\n      <td>Pending</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# 3. 🧠 Ask AI: What Should We Explore?","metadata":{}},{"cell_type":"markdown","source":"## prompt for the kaggle data set","metadata":{}},{"cell_type":"code","source":"# Prompt:\nsample = data.head(5).to_markdown()  # Only show a small sample in the prompt\nprompt = f\"\"\"\nHere is a a few rows of our dataset:\n\n{sample}\n\nBased on this dataset, what are some useful questions we should ask during further data analysis?\n\"\"\"\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents=prompt\n)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:54.353856Z","iopub.execute_input":"2025-04-22T05:31:54.354224Z","iopub.status.idle":"2025-04-22T05:31:55.989804Z","shell.execute_reply.started":"2025-04-22T05:31:54.354188Z","shell.execute_reply":"2025-04-22T05:31:55.988858Z"}},"outputs":[{"name":"stdout","text":"Okay, based on the provided dataset columns, here are some useful questions to ask during further data analysis. I've grouped them by category for clarity:\n\n**Sales & Revenue:**\n\n*   **Overall Sales Performance:**\n    *   What is the total revenue generated?\n    *   What is the average order value?\n    *   What is the distribution of order values?\n    *   What are the monthly/quarterly/yearly sales trends? Are sales increasing, decreasing, or stagnant?\n*   **Product Performance:**\n    *   Which products generate the most revenue?\n    *   Which products are most frequently ordered (highest quantity sold)?\n    *   What is the average quantity sold per product?\n    *   Are there any products that are consistently underperforming?\n*   **Category Performance:**\n    *   Which product category generates the most revenue?\n    *   Which category has the highest average order value?\n    *   \n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Prompt for the input data","metadata":{}},{"cell_type":"code","source":"# Prompt:\nsample_1 = df.head(5).to_markdown()\n\nprompt_1 = f\"\"\"\nHere is a few rows of our dataset:\n\n{sample_1}\n\nBased on this dataset, what are some useful questions we should ask during further data analysis?\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents=prompt_1\n)\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:55.990731Z","iopub.execute_input":"2025-04-22T05:31:55.991065Z","iopub.status.idle":"2025-04-22T05:31:57.587071Z","shell.execute_reply.started":"2025-04-22T05:31:55.991039Z","shell.execute_reply":"2025-04-22T05:31:57.586246Z"}},"outputs":[{"name":"stdout","text":"Okay, based on the provided dataset structure, here are some useful questions to ask during further data analysis, categorized for clarity:\n\n**I. Sales Performance & Trends:**\n\n*   **Overall Sales:**\n    *   What is the total sales revenue generated?\n    *   What is the average order value?\n    *   What is the distribution of order sizes (quantity)?\n*   **Temporal Analysis:**\n    *   What are the sales trends over time (daily, weekly, monthly)?  (Requires more data over a longer period)\n    *   Are there any seasonal patterns in sales? (Requires more data over a longer period)\n    *   Are there any specific days or periods with unusually high or low sales?\n*   **Product Performance:**\n    *   Which product is the best seller?\n    *   Which category generates the most revenue?\n    *   What is the average quantity sold per product?\n    *   Are there\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 4. 🧾 Generate & Run SQL Queries from Natural Language","metadata":{}},{"cell_type":"code","source":"# description function for both kaggle and user inputed csv:\ndef describe_table(conn, table_name: str):\n    cursor = conn.cursor()\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n    return [(col[1], col[2]) for col in cursor.fetchall()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.587869Z","iopub.execute_input":"2025-04-22T05:31:57.588178Z","iopub.status.idle":"2025-04-22T05:31:57.593177Z","shell.execute_reply.started":"2025-04-22T05:31:57.58815Z","shell.execute_reply":"2025-04-22T05:31:57.592295Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Query function for both kaggle and user inputed csv: \ndef execute_query(conn, sql: str) -> list[list[str]]:\n    print(f' - DB CALL: execute_query({sql})')\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    return cursor.fetchall()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.594187Z","iopub.execute_input":"2025-04-22T05:31:57.594448Z","iopub.status.idle":"2025-04-22T05:31:57.611141Z","shell.execute_reply.started":"2025-04-22T05:31:57.594429Z","shell.execute_reply":"2025-04-22T05:31:57.610232Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## running SQL Queries for the kaggle data set","metadata":{}},{"cell_type":"code","source":"# Using sqlite3 to create the database\nkaggle_conn = sqlite3.connect(\"sample.db\")\ndata.columns = [col.strip().replace(\" \", \"_\") for col in data.columns]\ndata.to_sql(\"data\", kaggle_conn, if_exists=\"replace\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.612072Z","iopub.execute_input":"2025-04-22T05:31:57.612376Z","iopub.status.idle":"2025-04-22T05:31:57.65578Z","shell.execute_reply.started":"2025-04-22T05:31:57.612351Z","shell.execute_reply":"2025-04-22T05:31:57.654982Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"250"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"describe_table(kaggle_conn, \"data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.656782Z","iopub.execute_input":"2025-04-22T05:31:57.65707Z","iopub.status.idle":"2025-04-22T05:31:57.66323Z","shell.execute_reply.started":"2025-04-22T05:31:57.657051Z","shell.execute_reply":"2025-04-22T05:31:57.662294Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[('Order_ID', 'TEXT'),\n ('Date', 'TEXT'),\n ('Product', 'TEXT'),\n ('Category', 'TEXT'),\n ('Price', 'INTEGER'),\n ('Quantity', 'INTEGER'),\n ('Total_Sales', 'INTEGER'),\n ('Customer_Name', 'TEXT'),\n ('Customer_Location', 'TEXT'),\n ('Payment_Method', 'TEXT'),\n ('Status', 'TEXT')]"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"execute_query(kaggle_conn,\"select * from data where Category == 'Footwear'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.664448Z","iopub.execute_input":"2025-04-22T05:31:57.664896Z","iopub.status.idle":"2025-04-22T05:31:57.682864Z","shell.execute_reply.started":"2025-04-22T05:31:57.664873Z","shell.execute_reply":"2025-04-22T05:31:57.682041Z"}},"outputs":[{"name":"stdout","text":" - DB CALL: execute_query(select * from data where Category == 'Footwear')\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[('ORD0001',\n  '14-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  3,\n  180,\n  'Emma Clark',\n  'New York',\n  'Debit Card',\n  'Cancelled'),\n ('ORD0003',\n  '15-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'John Doe',\n  'Denver',\n  'Amazon Pay',\n  'Cancelled'),\n ('ORD0004',\n  '19-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  3,\n  180,\n  'Olivia Wilson',\n  'Dallas',\n  'Credit Card',\n  'Pending'),\n ('ORD0019',\n  '22-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  3,\n  180,\n  'Olivia Wilson',\n  'Houston',\n  'Credit Card',\n  'Completed'),\n ('ORD0046',\n  '06-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'David Lee',\n  'Houston',\n  'Debit Card',\n  'Cancelled'),\n ('ORD0053',\n  '24-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  4,\n  240,\n  'Emily Johnson',\n  'Los Angeles',\n  'PayPal',\n  'Completed'),\n ('ORD0079',\n  '09-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'Emily Johnson',\n  'Denver',\n  'Gift Card',\n  'Cancelled'),\n ('ORD0080',\n  '23-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  4,\n  240,\n  'Sophia Miller',\n  'San Francisco',\n  'Debit Card',\n  'Pending'),\n ('ORD0087',\n  '13-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  5,\n  300,\n  'Emma Clark',\n  'Miami',\n  'Debit Card',\n  'Completed'),\n ('ORD0089',\n  '26-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  5,\n  300,\n  'Emma Clark',\n  'Los Angeles',\n  'Credit Card',\n  'Cancelled'),\n ('ORD0100',\n  '13-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  1,\n  60,\n  'Jane Smith',\n  'Houston',\n  'Gift Card',\n  'Cancelled'),\n ('ORD0114',\n  '23-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  1,\n  60,\n  'Emma Clark',\n  'Houston',\n  'Credit Card',\n  'Pending'),\n ('ORD0115',\n  '21-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  3,\n  180,\n  'Olivia Wilson',\n  'Miami',\n  'Amazon Pay',\n  'Completed'),\n ('ORD0147',\n  '05-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  5,\n  300,\n  'Jane Smith',\n  'Dallas',\n  'PayPal',\n  'Pending'),\n ('ORD0149',\n  '20-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  3,\n  180,\n  'Michael Brown',\n  'New York',\n  'Gift Card',\n  'Pending'),\n ('ORD0163',\n  '20-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'Chris White',\n  'Houston',\n  'PayPal',\n  'Completed'),\n ('ORD0181',\n  '03-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'David Lee',\n  'Los Angeles',\n  'Debit Card',\n  'Cancelled'),\n ('ORD0190',\n  '23-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  1,\n  60,\n  'Sophia Miller',\n  'Houston',\n  'PayPal',\n  'Completed'),\n ('ORD0197',\n  '12-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'Michael Brown',\n  'Denver',\n  'Gift Card',\n  'Completed'),\n ('ORD0204',\n  '08-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  5,\n  300,\n  'Jane Smith',\n  'Miami',\n  'Gift Card',\n  'Pending'),\n ('ORD0215',\n  '07-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  1,\n  60,\n  'Emma Clark',\n  'Houston',\n  'PayPal',\n  'Completed'),\n ('ORD0217',\n  '19-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'Emma Clark',\n  'Boston',\n  'Gift Card',\n  'Pending'),\n ('ORD0226',\n  '05-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  4,\n  240,\n  'Chris White',\n  'Chicago',\n  'Amazon Pay',\n  'Completed'),\n ('ORD0228',\n  '12-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  1,\n  60,\n  'David Lee',\n  'San Francisco',\n  'Credit Card',\n  'Pending'),\n ('ORD0229',\n  '21-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  3,\n  180,\n  'Emma Clark',\n  'San Francisco',\n  'Credit Card',\n  'Pending'),\n ('ORD0233',\n  '20-02-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  1,\n  60,\n  'David Lee',\n  'Boston',\n  'PayPal',\n  'Cancelled'),\n ('ORD0243',\n  '05-03-25',\n  'Running Shoes',\n  'Footwear',\n  60,\n  2,\n  120,\n  'Chris White',\n  'Houston',\n  'Amazon Pay',\n  'Completed')]"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Running SQL Queries from the input data","metadata":{}},{"cell_type":"code","source":"# 🛢️ Save uploaded data to SQLite\nuser_conn = sqlite3.connect(\"sample_1.db\")\ndf.columns = [col.strip().replace(\" \", \"_\") for col in df.columns]\ndf.to_sql(\"df\", user_conn, if_exists=\"replace\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.683734Z","iopub.execute_input":"2025-04-22T05:31:57.683955Z","iopub.status.idle":"2025-04-22T05:31:57.729273Z","shell.execute_reply.started":"2025-04-22T05:31:57.683937Z","shell.execute_reply":"2025-04-22T05:31:57.728422Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"250"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"describe_table(user_conn,\"df\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.730053Z","iopub.execute_input":"2025-04-22T05:31:57.730282Z","iopub.status.idle":"2025-04-22T05:31:57.736508Z","shell.execute_reply.started":"2025-04-22T05:31:57.730263Z","shell.execute_reply":"2025-04-22T05:31:57.735731Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[('Order_ID', 'TEXT'),\n ('Date', 'TEXT'),\n ('Product', 'TEXT'),\n ('Category', 'TEXT'),\n ('Price', 'INTEGER'),\n ('Quantity', 'INTEGER'),\n ('Total_Sales', 'INTEGER'),\n ('Customer_Name', 'TEXT'),\n ('Customer_Location', 'TEXT'),\n ('Payment_Method', 'TEXT'),\n ('Status', 'TEXT')]"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"\"\"\"Uncomment the line below to execute a query on the user-uploaded dataset.\"\"\"\n# execute_query()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.737419Z","iopub.execute_input":"2025-04-22T05:31:57.737674Z","iopub.status.idle":"2025-04-22T05:31:57.758266Z","shell.execute_reply.started":"2025-04-22T05:31:57.737651Z","shell.execute_reply":"2025-04-22T05:31:57.757527Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'Uncomment the line below to execute a query on the user-uploaded dataset.'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"print(\"📂 Default dataset loaded into SQLite as 'data'\")\nprint(\"📂 User-uploaded dataset loaded into SQLite as 'df'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.759284Z","iopub.execute_input":"2025-04-22T05:31:57.759628Z","iopub.status.idle":"2025-04-22T05:31:57.775537Z","shell.execute_reply.started":"2025-04-22T05:31:57.759605Z","shell.execute_reply":"2025-04-22T05:31:57.774703Z"}},"outputs":[{"name":"stdout","text":"📂 Default dataset loaded into SQLite as 'data'\n📂 User-uploaded dataset loaded into SQLite as 'df'\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Agents","metadata":{}},{"cell_type":"code","source":"# Agents function for both Kaggle and user-uploaded CSV:\ndef agent_loop(df, conn, table_name=\"data\"):\n    print(\"🔍 Ask a question about the dataset (or type 'exit'):\")\n\n    # Dynamically build schema from df\n    schema = f\"Table: {table_name}\\nColumns:\\n\"\n    for col in df.columns:\n        schema += f\"- {col}\\n\"\n\n    # Start interaction loop\n    while True:\n        user_input = input(\"\\n🧍 You: \")\n        if user_input.lower() == \"exit\":\n            break\n\n        # Prompt for SQL\n        prompt = f\"\"\"\n        You are a helpful assistant that answers data questions by generating SQL queries.\n        Here is the table schema:\n        {schema}\n        Question: {user_input}\n        Only respond with a valid SQL query.\n        \"\"\"\n        \n        # Gemini API call using the Kaggle client\n        response = client.models.generate_content(  # Make sure `client` is initialized properly\n            model='gemini-2.0-flash',\n            config=short_config,\n            contents=prompt\n        )\n        sql = response.text.strip().replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n        print(f\"\\n🧾 Cleaned SQL:\\n{sql}\")\n\n        try:\n            result = execute_query(conn, sql)  # Assuming `conn` is a valid connection object\n            print(\"\\n📊 Query Results:\")\n            for row in result:\n                print(row)\n\n            # Explanation\n            summary_prompt = f\"\"\"\n            Here is the result of the SQL query:\n            {result}\n            Explain this result in plain English for a data analyst.\n            \"\"\"\n            summary = client.models.generate_content(  # Again, using the Kaggle client here\n                model='gemini-2.0-flash',\n                config=short_config,\n                contents=summary_prompt\n            )\n            print(f\"\\n🗣️ Summary:\\n{summary.text}\")\n        except Exception as e:\n            print(f\"❌ Error: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.776615Z","iopub.execute_input":"2025-04-22T05:31:57.777127Z","iopub.status.idle":"2025-04-22T05:31:57.796603Z","shell.execute_reply.started":"2025-04-22T05:31:57.777104Z","shell.execute_reply":"2025-04-22T05:31:57.795757Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## 5. 🤖 Agent Mode: Chat with Your Data W kaggle data and input data","metadata":{}},{"cell_type":"code","source":"agent_loop(data, kaggle_conn) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:31:57.797606Z","iopub.execute_input":"2025-04-22T05:31:57.79795Z","iopub.status.idle":"2025-04-22T05:32:53.320241Z","shell.execute_reply.started":"2025-04-22T05:31:57.797928Z","shell.execute_reply":"2025-04-22T05:32:53.319456Z"}},"outputs":[{"name":"stdout","text":"🔍 Ask a question about the dataset (or type 'exit'):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🧍 You:  What are the average sales per order?\n"},{"name":"stdout","text":"\n🧾 Cleaned SQL:\nSELECT AVG(Total_Sales) AS Average_Sales_Per_Order FROM data;\n - DB CALL: execute_query(SELECT AVG(Total_Sales) AS Average_Sales_Per_Order FROM data;)\n\n📊 Query Results:\n(975.38,)\n\n🗣️ Summary:\nOkay, here's the explanation of the SQL query result `[(975.38,)]` in plain English for a data analyst:\n\n**\"The query you ran returned a single row containing a single value, which is 975.38.  The trailing comma and parentheses indicate that this value is part of a tuple (a container holding one or more values), even though it only contains one number in this case.\"**\n\n**In simpler terms:**\n\n\"The SQL query resulted in just one number: 975.38.  The parentheses and comma are just SQL's way of showing that it's technically a single-element list, even though it's just one number.\"\n\n**Possible Implications (what you might do next):**\n\nAs a data analyst, you'll want to consider the context of the query to understand the meaning of this value. Here are some examples:\n\n*   **Average Order Value\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🧍 You:  exit\n"}],"execution_count":23},{"cell_type":"code","source":"\"\"\"Uncomment the line below to execute the agent_loop on the user-uploaded dataset.\"\"\"\n# agent_loop(df, user_conn, table_name=\"df\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:32:57.866776Z","iopub.execute_input":"2025-04-22T05:32:57.867089Z","iopub.status.idle":"2025-04-22T05:33:25.068004Z","shell.execute_reply.started":"2025-04-22T05:32:57.867069Z","shell.execute_reply":"2025-04-22T05:33:25.067044Z"}},"outputs":[{"name":"stdout","text":"🔍 Ask a question about the dataset (or type 'exit'):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🧍 You:  What is the total sales revenue generated?\n"},{"name":"stdout","text":"\n🧾 Cleaned SQL:\nSELECT sum(Total_Sales) FROM df\n - DB CALL: execute_query(SELECT sum(Total_Sales) FROM df)\n\n📊 Query Results:\n(243845,)\n\n🗣️ Summary:\nOkay, here's the explanation for a data analyst in plain English:\n\n\"The query you ran returned a single row with one column. The value in that column is 243845.  Without knowing the query itself, it's impossible to be 100% sure what this means, but the query likely aggregated or counted something, and the result is the number 243845.\"\n\n**Example scenarios (depending on the query):**\n\n*   **Total number of customers:** \"There are 243,845 customers in the customer table.\"\n*   **Total sales revenue (maybe in dollars):** \"The total sales revenue is $243,845.\"\n*   **Count of orders:** \"There are 243,845 orders in the database.\"\n*   **Number of products in a specific category:** \"There are 243,84\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🧍 You:  exit\n"}],"execution_count":25}]}